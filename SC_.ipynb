{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import BertTokenizer, BertModel, AdamW\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load the Quora Question Pairs dataset\n",
        "dataset = load_dataset(\"quora\")\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertModel.from_pretrained(model_name)\n",
        "\n",
        "# Function to encode a sentence into embeddings\n",
        "def encode_sentence(sentence):\n",
        "    inputs = tokenizer(sentence, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
        "\n",
        "# Compute similarity between two sentences\n",
        "def calculate_similarity(sentence1, sentence2):\n",
        "    embedding1 = encode_sentence(sentence1)\n",
        "    embedding2 = encode_sentence(sentence2)\n",
        "    similarity = cosine_similarity([embedding1], [embedding2])\n",
        "    return similarity[0][0]\n",
        "\n",
        "# Define a custom Dataset class\n",
        "class QuoraDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length=128):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        question1 = self.data[idx]['question1']\n",
        "        question2 = self.data[idx]['question2']\n",
        "        label = self.data[idx]['is_duplicate']\n",
        "\n",
        "        # Tokenize both questions\n",
        "        inputs = self.tokenizer(\n",
        "            question1, question2, truncation=True, padding=True, max_length=self.max_length, return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': inputs['input_ids'].squeeze(),\n",
        "            'attention_mask': inputs['attention_mask'].squeeze(),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Training function\n",
        "def train_model(model, train_loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch in tqdm(train_loader, desc=\"Training\", ncols=100):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.pooler_output\n",
        "\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, preds = torch.max(logits, dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    accuracy = correct / total\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_model(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader, desc=\"Evaluating\", ncols=100):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.pooler_output\n",
        "\n",
        "            loss = criterion(logits, labels)\n",
        "            total_loss += loss.item()\n",
        "            _, preds = torch.max(logits, dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(val_loader)\n",
        "    accuracy = correct / total\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "# Data preprocessing and preparation\n",
        "def prepare_data():\n",
        "    # Convert to pandas dataframe\n",
        "    data = dataset['train']\n",
        "    data = pd.DataFrame(data)\n",
        "\n",
        "    # Split into train and validation sets\n",
        "    train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = QuoraDataset(train_data.to_dict('records'), tokenizer)\n",
        "    val_dataset = QuoraDataset(val_data.to_dict('records'), tokenizer)\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=16)\n",
        "\n",
        "    return train_loader, val_loader\n",
        "\n",
        "# Fine-tune BERT model\n",
        "def fine_tune_model():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    # Set optimizer and loss function\n",
        "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    # Prepare data\n",
        "    train_loader, val_loader = prepare_data()\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(3):\n",
        "        print(f\"Epoch {epoch+1}\")\n",
        "        train_loss, train_accuracy = train_model(model, train_loader, optimizer, criterion, device)\n",
        "        val_loss, val_accuracy = evaluate_model(model, val_loader, criterion, device)\n",
        "\n",
        "        print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
        "        print(f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "    # Save the fine-tuned model\n",
        "    torch.save(model.state_dict(), \"fine_tuned_bert_model.pth\")\n",
        "    print(\"Model saved.\")\n",
        "\n",
        "# Example to calculate similarity on a few question pairs\n",
        "def calculate_sample_similarity():\n",
        "    question1 = dataset['train'][0]['question1']\n",
        "    question2 = dataset['train'][0]['question2']\n",
        "    label = dataset['train'][0]['is_duplicate']\n",
        "\n",
        "    print(f\"Question 1: {question1}\")\n",
        "    print(f\"Question 2: {question2}\")\n",
        "    print(f\"Ground Truth Label (1=Duplicate, 0=Not): {label}\")\n",
        "\n",
        "    similarity = calculate_similarity(question1, question2)\n",
        "    print(f\"Cosine Similarity Score: {similarity:.4f}\")\n",
        "\n",
        "# Main function to train and evaluate the model\n",
        "def main():\n",
        "    # Uncomment the line below to fine-tune the model\n",
        "    # fine_tune_model()\n",
        "\n",
        "    # Evaluate a sample similarity\n",
        "    calculate_sample_similarity()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "QsNh-pA0v4P-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}